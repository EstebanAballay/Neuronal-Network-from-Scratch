# Red Neuronal Desde Cero (Pass/Fail Predictor) üß†

Este proyecto implementa una **Red Neuronal Artificial (ANN)** construida puramente en **Python** y **NumPy**, sin utilizar frameworks de aprendizaje profundo como TensorFlow, Keras o PyTorch.

El objetivo del modelo es predecir si un estudiante **aprobar√° o reprobar√°** una materia bas√°ndose en dos variables:
1. Cantidad de clases asistidas.
2. Horas de estudio dedicadas.

## üìã Tabla de Contenidos
- [Caracter√≠sticas](#-caracter√≠sticas)
- [Arquitectura de la Red](#-arquitectura-de-la-red)
- [Pre-requisitos](#-pre-requisitos)
- [Instalaci√≥n y Uso](#-instalaci√≥n-y-uso)
- [Estructura del Proyecto](#-estructura-del-proyecto)
- [Fundamentos Matem√°ticos](#-fundamentos-matem√°ticos)

## ‚ú® Caracter√≠sticas
- **Zero Frameworks:** Toda la l√≥gica de *Forward Propagation*, *Backpropagation* y *Gradient Descent* est√° implementada manualmente.
- **Visualizaci√≥n de Datos:** Gr√°ficos con `matplotlib` para entender la distribuci√≥n del dataset.
- **Predicci√≥n Interactiva:** Script para probar el modelo con datos ingresados por el usuario.
- **Optimizaci√≥n:** Uso de funci√≥n de activaci√≥n Sigmoide y p√©rdida Binary Cross Entropy.

## üèó Arquitectura de la Red
El modelo es un Perceptr√≥n Multicapa (MLP) con la siguiente topolog√≠a:

* **Capa de Entrada:** 2 neuronas (Clases, Horas de estudio).
* **Capa Oculta:** 3 neuronas (con pesos inicializados aleatoriamente).
* **Capa de Salida:** 1 neurona (Probabilidad de aprobar).
* **Funci√≥n de Activaci√≥n:** Sigmoide (`1 / (1 + e^-z)`).
* **Bias:** Integrado en el c√°lculo matricial (`+ 1` en la suma ponderada).

## üõ† Pre-requisitos
El proyecto requiere Python 3.x y las siguientes librer√≠as para manejo de matrices y gr√°ficos:
```bash
pip install numpy matplotlib
```
## üöÄ Instalaci√≥n

1. Clonar el repositorio:
```bash
git clone [https://github.com/tu-usuario/repo.git](https://github.com/tu-usuario/repo.git)
```

2. Visualizar el Dataset:
```bash
python Dataset.py
```
O simplemente darle a "ejecutar" en su editor de c√≥digo

3.Entrenar el Modelo: Para ejecutar el algoritmo de entrenamiento y ver c√≥mo disminuye la p√©rdida (Loss) iteraci√≥n tras iteraci√≥n:
```bash
python Training.py
```

4.Probar el Modelo (Predicci√≥n): Para interactuar con la red neuronal utilizando los pesos entrenados:
```bash
python Testing.py
```

##üìÇ Estructura del Proyecto
- Dataset.py: Contiene el diccionario de datos de entrenamiento y la l√≥gica para graficar los puntos en un plano 2D.

- Training.py: El n√∫cleo del proyecto. Contiene la clase neuron, la funci√≥n de p√©rdida y el bucle principal que ejecuta el Descenso del Gradiente para ajustar los pesos.

- Testing.py: Utiliza los pesos √≥ptimos obtenidos del entrenamiento para realizar inferencias sobre nuevos datos introducidos por consola.

##üßÆ Fundamentos Matem√°ticos
+ Forward Propagation: Se calcula el producto punto de las entradas por los pesos y se pasa por la funci√≥n de activaci√≥n: $$ z = (Inputs \cdot Weights) + 1 $$ $$ \sigma(z) = \frac{1}{1 + e^{-z}} $$

+ Funci√≥n de Costo (Loss): Se utiliza la Entrop√≠a Cruzada Binaria (Binary Cross Entropy) para medir el error: $$ Loss = -\frac{1}{N} \sum (y \cdot \log(\hat{y}) + (1-y) \cdot \log(1-\hat{y})) $$

+ Backpropagation: Se calculan las derivadas parciales del error respecto a cada peso utilizando la Regla de la Cadena para actualizar los pesos en la direcci√≥n opuesta al gradiente: $$ W_{nuevo} = W_{actual} - (learning_rate \cdot \frac{\partial Error}{\partial W}) $$
